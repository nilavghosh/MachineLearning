{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "AV-AzsZdpaHQ",
    "outputId": "965db2a6-6fa2-4638-fb00-77e77546213c"
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "# !pip3 install colorama\n",
    "from colorama import Fore\n",
    "%matplotlib inline\n",
    "# import colorama\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "tehNn-eVfZ-Z",
    "outputId": "ea771a2e-b0d5-458b-e072-5142fc1748eb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  IMPORT FROM CSV\n",
    "'''\n",
    "domestic, worldwide = True, False\n",
    "\n",
    "\n",
    "# # path = \"../../multi_thread_scraper/feature_engineering/\"\n",
    "# path = \"../../multi_thread_scraper/data/\"\n",
    "# # path = \"/content/gdrive/My Drive/extensive_features/test\"\n",
    "# if domestic:\n",
    "# #   df = pd.read_csv('{}domestic_engineered_2000_adjusted.csv'.format(path))\n",
    "#   df = pd.read_csv('{}full_dataset.csv'.format(path))\n",
    "# else:\n",
    "#   df = pd.read_csv('{}international_engineered_2000.csv'.format(path))\n",
    "  \n",
    "df = pd.read_csv('{}full_dataset.csv'.format(path))\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.replace(\"[]\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if domestic:\n",
    "#   df = df.reset_index()\n",
    "#   new_office= []\n",
    "#   for bo in df['dom_box_office']:\n",
    "#     if bo > 0:\n",
    "#       if bo <= 200000000:\n",
    "#         new_office.append(bo)\n",
    "#       else:\n",
    "#         new_office.append(np.nan)\n",
    "#     else:\n",
    "#       new_office.append(np.nan)\n",
    "#   df['dom_box_office'] = new_office\n",
    "#   for idx, row in enumerate(df.itertuples()):\n",
    "#     if row.dom_box_office > 0:\n",
    "#       continue\n",
    "#     else:\n",
    "#       df = df.drop(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5955, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'box_office', 'mpaa', 'release', 'budget', 'theatres',\n",
       "       'runtime', 'sequel', 'G', 'NC-17', 'PG', 'PG-13', 'R', 'Unrated',\n",
       "       'date', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>sequel</th>\n",
       "      <th>theatres</th>\n",
       "      <th>G</th>\n",
       "      <th>NC-17</th>\n",
       "      <th>PG</th>\n",
       "      <th>PG-13</th>\n",
       "      <th>R</th>\n",
       "      <th>Unrated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10484</th>\n",
       "      <td>10000000.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10486</th>\n",
       "      <td>2000000.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10487</th>\n",
       "      <td>17000000.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10490</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>250000.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           budget  runtime  sequel  theatres    G  NC-17   PG  PG-13    R  \\\n",
       "10484  10000000.0     92.0     0.0       1.0  0.0    0.0  1.0    0.0  0.0   \n",
       "10486   2000000.0     97.0     0.0       1.0  0.0    0.0  0.0    0.0  1.0   \n",
       "10487  17000000.0    105.0     0.0       2.0  0.0    0.0  0.0    1.0  0.0   \n",
       "10490    300000.0     90.0     0.0       3.0  0.0    0.0  0.0    0.0  1.0   \n",
       "10497    250000.0     78.0     0.0       1.0  0.0    0.0  0.0    0.0  0.0   \n",
       "\n",
       "       Unrated  \n",
       "10484      0.0  \n",
       "10486      0.0  \n",
       "10487      0.0  \n",
       "10490      0.0  \n",
       "10497      1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if domestic:\n",
    "  columns = ['budget', 'runtime', 'sequel',\n",
    "       'theatres', 'G', 'NC-17', 'PG', 'PG-13', 'R', 'Unrated']\n",
    "  X = df[columns]\n",
    "  for val in columns:\n",
    "    try:\n",
    "      X[val] = X[val].astype(np.double)\n",
    "    except:\n",
    "      print(val)\n",
    "else:\n",
    "  columns = ['budget', 'mpaa', 'num_theatres',\n",
    "       'runtime','act_dir_recent', 'act_gross_rec',\n",
    "       'dir_gross_rec',\n",
    "       'star_value_recent', 'comp_high', 'comp_med', 'comp_low', 'avg_tenure',\n",
    "       'total_tenure', 'sw_rec', 'prev_year_avg', 'ytd_avg', '7_day_comp',\n",
    "       'prev_genre_mpaa', 'ytd_genre_mpaa', 'rec_genre_avg', 'act_gen_rec',\n",
    "       'sequel_last', 'seasonality',\n",
    "       'rec_studio', 'rec_spec_eff', 'transmedia']\n",
    "  X = df[columns]\n",
    "  for val in columns:\n",
    "    try:\n",
    "      X[val] = X[val].astype(np.double)\n",
    "    except:\n",
    "      print(val)\n",
    "    \n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuMbyCYVfiBL"
   },
   "outputs": [],
   "source": [
    "y_train_regression = df['box_office']\n",
    "y_train_regression= y_train_regression.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Ky-VYZMzjatt",
    "outputId": "38028820-fa77-4f74-eae7-d6a065ce8046"
   },
   "outputs": [],
   "source": [
    "len(y_train_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.fillna(X.mean())\n",
    "# X = X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "SQyCSGB9fifh",
    "outputId": "eeb34e56-f012-4059-a259-d9b784b3ac3c"
   },
   "outputs": [],
   "source": [
    "y_train_regression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "FHlNHZjafio3",
    "outputId": "3c4c4e84-d09b-49fe-8320-b031649cb5d9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "if domestic:\n",
    "  for value in y_train_regression:\n",
    "      value = float(value)\n",
    "  #     print (type(value))\n",
    "      if value <= 1000000:\n",
    "  #         y.append(0)\n",
    "  #     elif value > 1000000 and value <= 10000000:\n",
    "          y.append(0)\n",
    "      elif value > 1000000 and value <= 10000000:\n",
    "          y.append(1)\n",
    "      elif value > 10000000 and value <= 20000000:\n",
    "          y.append(2)\n",
    "      elif value > 20000000 and value <= 40000000:\n",
    "          y.append(3)\n",
    "      elif value > 40000000 and value <= 65000000:\n",
    "          y.append(4)\n",
    "      elif value > 65000000 and value <= 100000000:\n",
    "          y.append(5)\n",
    "      elif value > 100000000 and value <= 150000000:\n",
    "          y.append(6)\n",
    "      elif value > 150000000 and value <= 200000000:\n",
    "          y.append(7)\n",
    "      elif value > 200000000:\n",
    "          y.append(8)\n",
    "else:\n",
    "    for value in y_train_regression:\n",
    "      value = float(value)\n",
    "  #     print (type(value))\n",
    "      if value <= 10000000:\n",
    "          y.append(0)\n",
    "      elif value > 10000000 and value <= 50000000:\n",
    "          y.append(1)\n",
    "      elif value > 50000000 and value <= 100000000:\n",
    "          y.append(2)\n",
    "      elif value > 100000000 and value <= 200000000:\n",
    "          y.append(3)\n",
    "      elif value > 200000000 and value <= 400000000:\n",
    "          y.append(4)\n",
    "      elif value > 400000000 and value <= 600000000:\n",
    "          y.append(5)\n",
    "      elif value > 600000000 and value <= 800000000:\n",
    "          y.append(6)\n",
    "      elif value > 800000000 and value <= 1000000000:\n",
    "          y.append(7)\n",
    "      elif value > 1000000000:\n",
    "          y.append(8)\n",
    "      else:\n",
    "        print(value)\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "MBkCGyipaRUU",
    "outputId": "7537b3a0-b20d-4558-a043-79e50149186e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSES\n",
      "0: 905\n",
      "1: 1269\n",
      "2: 850\n",
      "3: 1042\n",
      "4: 733\n",
      "5: 460\n",
      "6: 351\n",
      "7: 148\n",
      "8: 197\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASSES\")\n",
    "print(f\"0: {y.count(0)}\")\n",
    "print(f\"1: {y.count(1)}\")\n",
    "print(f\"2: {y.count(2)}\")\n",
    "print(f\"3: {y.count(3)}\")\n",
    "print(f\"4: {y.count(4)}\")\n",
    "print(f\"5: {y.count(5)}\")\n",
    "print(f\"6: {y.count(6)}\")\n",
    "print(f\"7: {y.count(7)}\")\n",
    "print(f\"8: {y.count(8)}\")\n",
    "\n",
    "# print(\"BUDGETS\")\n",
    "# budgets = []\n",
    "# for val in X['budget']:\n",
    "#   budgets.append(val)\n",
    "# budgets.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5955"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BTwFGGvfixV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_df = X\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5955"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4NXaqt9fi6D"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SCALE DATASET\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_temp = scaler.transform(X)\n",
    "\n",
    "# SPLIT INTO TRAIN/VALID/TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_temp, y, test_size = 0.2, random_state = 0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size= 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnTzvNlRfjC5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmykCjaGfrUq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Create tensors for our train/valid/test sets\n",
    "  Need variable to accumulate gradients. \n",
    "  First we create tensor, then we will create variable \n",
    "'''\n",
    "# Numpy to Tensor Conversion (Train Set)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# Numpy to Tensor Conversion (Valid Set)\n",
    "X_valid = torch.from_numpy(X_valid)\n",
    "y_valid = torch.from_numpy(y_valid)\n",
    "\n",
    "# Numpy to Tensor Conversion (Test Set)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "sgt3mXS5frd5",
    "outputId": "315b86f0-06fb-4939-bcfc-c45fe4858ad5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_74mdzScfrnG"
   },
   "outputs": [],
   "source": [
    "# Make torch datasets from train / valid / test sets\n",
    "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "valid = torch.utils.data.TensorDataset(X_valid,y_valid)\n",
    "test = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "\n",
    "# Create train / valid / test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlaiYnrCfrv0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.batchnorm as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OnOHEg2fr4q"
   },
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim = 39, output_dim = 9, dropout_rate = .2, act = 'lrelu'):\n",
    "        super(ANN, self).__init__()\n",
    "    \n",
    "        self.input_dim = input_dim\n",
    "        self.act = act\n",
    "        # Input Layer (2) -> 784\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        # 256 -> 128\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        # 128 -> 128\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        # 32 -> output layer(10)\n",
    "        self.output_layer = nn.Linear(4,9)\n",
    "        # Dropout Layer (20%) to reduce overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        #input features\n",
    "        self.batchnorm1 = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "    \n",
    "    \n",
    "    # Feed Forward Function\n",
    "    def forward(self, x):\n",
    "      if self.act == 'lrelu':\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "#         x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.leaky_relu(self.fc5(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc6(x))\n",
    "        x = F.leaky_relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "      elif self.act == 'relu':\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "#         x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "      elif self.act == 'elu':\n",
    "        x = F.elu(self.fc1(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.elu(self.fc6(x))\n",
    "        x = F.elu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "      elif self.act == 'selu':\n",
    "        x = F.selu(self.fc1(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.selu(self.fc2(x))\n",
    "        x = F.selu(self.fc3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.selu(self.fc4(x))\n",
    "        x = F.selu(self.fc5(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.selu(self.fc6(x))\n",
    "        x = F.selu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "      elif self.act == 'celu':\n",
    "        x = F.celu(self.fc1(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.celu(self.fc2(x))\n",
    "        x = F.celu(self.fc3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.celu(self.fc4(x))\n",
    "        x = F.celu(self.fc5(x))\n",
    "      #         x = self.dropout(x)\n",
    "        x = F.celu(self.fc6(x))\n",
    "        x = F.celu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "        # Return the created model\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "wKUErcrifsB3",
    "outputId": "2b3bf292-5245-4298-a95b-bbf4d20599f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (output_layer): Linear(in_features=4, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (batchnorm1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network Model\n",
    "model = ANN(input_dim = 10, output_dim = 9, act='relu')\n",
    "# Print its architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwhyjI7MfsK1"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# from torch.optim import nadam\n",
    "# from optim import nadam\n",
    "# Specify Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Specify Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "oU5hJeQJfjU9",
    "outputId": "ebc0a152-ad0f-4f04-8598-78ee27ebda6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91\n",
      "Training Loss: 5395.7193\t Val Loss: 581.2186\n",
      "Training Acc: 37.00\t Val Acc: 40.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Num Epochs\n",
    "epochs =100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "plt.figure()\n",
    "#define plot function\n",
    "def plot_function():\n",
    "    t_loss.append(train_loss)\n",
    "    v_loss.append(val_loss)\n",
    "    plt.plot(t_loss, label=\"Train\")\n",
    "    plt.plot(v_loss, label=\"Valid\")\n",
    "    plt.title('train vs validation loss')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "# Start epochs\n",
    "for epoch in range(epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    # Set the training mode ON -> Activate Dropout Layers\n",
    "    model.train() # prepare model for training\n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Load Train Tuples with Labels(Targets)\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Convert our feature and labels to Variables to accumulate Gradients\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.LongTensor)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate Training Accuracy \n",
    "        predicted = torch.max(output.data, 1)[1]        \n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "        \n",
    "        # calculate the loss\n",
    "#         loss = loss_fn(output, target)\n",
    "        loss = torch.sqrt(loss_fn(output, target))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average training loss over an epoch\n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    # Avg Accuracy\n",
    "    train_accuracy = 100 * correct / float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    train_acc_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "        \n",
    "    # Implement Validation like K-fold Cross-validation \n",
    "    # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "    model.eval() # Required for Evaluation/Test\n",
    "\n",
    "    # Calculate Test/Validation Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "\n",
    "            # Convert our images and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "            \n",
    "            # Predict Output\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Loss\n",
    "#             loss = loss_fn(output, target)\n",
    "            loss = torch.sqrt(loss_fn(output, target))\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "    \n",
    "    # calculate average training loss and accuracy over an epoch\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = 100 * correct/ float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    val_acc_list.append(val_accuracy)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        PLOT EPOCHS IN GRAPH\n",
    "    \"\"\"\n",
    "    if epoch % 10 == 0:\n",
    "#       if val_loss <= valid_loss_min:\n",
    "#           print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "#           valid_loss_min,\n",
    "#           val_loss))\n",
    "#           torch.save(model.state_dict(), 'model.pt')\n",
    "#           valid_loss_min = val_loss\n",
    "      print(\"Epoch: {}\".format(epoch+1))\n",
    "      print(\"Training Loss: {:.4f}\\t Val Loss: {:.4f}\".format(\n",
    "        train_loss,\n",
    "        val_loss\n",
    "      ))\n",
    "      print(\"Training Acc: {:.2f}\\t Val Acc: {:.2f}\".format(\n",
    "        train_accuracy,\n",
    "        val_accuracy\n",
    "      ))\n",
    "      plot_function()\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "#     path=\"/content/gdrive/My Drive/extensive_features/test\"\n",
    "\n",
    "#     clear_output(wait=True)\n",
    "    # Move to next epoch\n",
    "    epoch_list.append(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HXIq8bJdcXBn",
    "outputId": "ca3c09bc-fd1b-4479-f738-bf194d8ebcaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 1839.356204032898\n",
      "Test Accuracy tensor(36)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "accuracy_test = 0\n",
    "test_total = 0\n",
    "correct = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for data, target in test_loader:\n",
    "  # Convert our tuples and labels to Variables to accumulate Gradients\n",
    "  data = Variable(data).float()\n",
    "  target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "  # Predict Output\n",
    "  output = model(data)\n",
    "\n",
    "  # Calculate Loss\n",
    "  loss = loss_fn(output, target)\n",
    "  test_loss += loss.item()*data.size(0)\n",
    "  # Get predictions from the maximum value\n",
    "  predicted = torch.max(output.data, 1)[1]\n",
    "  for val in predicted:\n",
    "    y_pred.append(val)\n",
    "#   y_pred.append(predicted)\n",
    "  for val in target:\n",
    "    y_true.append(val)\n",
    "\n",
    "  # Total number of labels\n",
    "  test_total += len(target)\n",
    "\n",
    "  # Total correct predictions\n",
    "  correct += (predicted == target).sum()\n",
    "#   print(\"{}:{}\".format(predicted, target))\n",
    "  \n",
    "\n",
    "test_loss = np.mean(test_loss)\n",
    "accuracy_test = 100 * correct/ float(test_total)\n",
    "print('Test loss', test_loss)\n",
    "print('Test Accuracy', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "cTVhC9QFYjdy",
    "outputId": "4a88a10e-5c2f-4071-9e8a-270ae9f0d7b8"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "0BXuS9RGxvCF",
    "outputId": "444391bc-ac3f-40cd-f27c-00ffa17fc77d"
   },
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "  def get_test_accuracy(lr, dropout_rate, batch_size, max_epoch, act, opt, decay, ams_grad):\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size = batch_size, shuffle = True)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True)\n",
    "        model = ANN(input_dim = 24, output_dim = 10, dropout_rate = dropout_rate, act=act)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # Specify Optimizer\n",
    "\n",
    "        if opt == 'Adam':\n",
    "          optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay, amsgrad=ams_grad)\n",
    "        elif opt == 'Adamax':\n",
    "          optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=decay)\n",
    "        elif opt == 'LBFGS':\n",
    "          optimizer = optim.LBFGS(model.parameters(), lr=lr)\n",
    "        # initialize tracker for minimum validation loss\n",
    "        epochs = max_epoch\n",
    "\n",
    "        # initialize tracker for minimum validation loss\n",
    "        valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "        # Some lists to keep track of loss and accuracy during each epoch\n",
    "        epoch_list = []\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        train_acc_list = []\n",
    "        val_acc_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "          train_loss = 0.0\n",
    "          val_loss = 0.0\n",
    "\n",
    "          ###################\n",
    "          # train the model #\n",
    "          ###################\n",
    "          # Set the training mode ON -> Activate Dropout Layers\n",
    "          model.train() # prepare model for training\n",
    "          # Calculate Accuracy         \n",
    "          correct = 0\n",
    "          total = 0\n",
    "          for data, target in train_loader:\n",
    "\n",
    "            # Convert our feature and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Training Accuracy \n",
    "            predicted = torch.max(output.data, 1)[1]        \n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "          train_loss = np.mean(train_loss)\n",
    "\n",
    "          # Avg Accuracy\n",
    "          train_accuracy = 100 * correct / float(total)\n",
    "\n",
    "          # Put them in their list\n",
    "          train_acc_list.append(train_accuracy)\n",
    "          train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "          # Implement Validation like K-fold Cross-validation \n",
    "          # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "          model.eval() # Required for Evaluation/Test\n",
    "\n",
    "          # Calculate Test/Validation Accuracy         \n",
    "          correct = 0\n",
    "          total = 0\n",
    "          with torch.no_grad():\n",
    "              for data, target in valid_loader:\n",
    "\n",
    "                  # Convert our images and labels to Variables to accumulate Gradients\n",
    "                  data = Variable(data).float()\n",
    "                  target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "                  # Predict Output\n",
    "                  output = model(data)\n",
    "\n",
    "                  # Calculate Loss\n",
    "                  loss = loss_fn(output, target)\n",
    "                  val_loss += loss.item()*data.size(0)\n",
    "                  # Get predictions from the maximum value\n",
    "                  predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "                  # Total number of labels\n",
    "                  total += len(target)\n",
    "\n",
    "                  # Total correct predictions\n",
    "                  correct += (predicted == target).sum()\n",
    "\n",
    "          # calculate average training loss and accuracy over an epoch\n",
    "          val_loss = np.mean(val_loss)\n",
    "          val_accuracy = 100 * correct/ float(total)\n",
    "\n",
    "          # Put them in their list\n",
    "          val_acc_list.append(val_accuracy)\n",
    "          val_loss_list.append(val_loss)\n",
    "\n",
    "\n",
    "    #       GRAB TEST METRICS\n",
    "          test_loss = 0\n",
    "          accuracy_test = 0\n",
    "          test_total = 0\n",
    "          correct = 0\n",
    "          for data, target in test_loader:\n",
    "            # Convert our tuples and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # Predict Output\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "            # Total number of labels\n",
    "            test_total += len(target)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "\n",
    "          test_loss = np.mean(test_loss)\n",
    "          accuracy_test = 100 * correct/ float(test_total)\n",
    "\n",
    "          return accuracy_test\n",
    "\n",
    "  lrs = [.1, .01,.001, .0001]\n",
    "  activations = ['relu', 'elu', 'lrelu', 'selu', 'celu']\n",
    "  dropout_rates = [.2, .3, .5, .7]\n",
    "  batch_sizes = [16, 32, 64, 128]\n",
    "  max_epochs = list(range(10, 30))\n",
    "  decay_rates = [0, .1, .001, .0001]\n",
    "  ams_grad = [True, False]\n",
    "  optimizers = ['Adam', 'Adamax']\n",
    "\n",
    "  test_accuracy_max = 0\n",
    "  best_hyperparameters = {}\n",
    "\n",
    "    # 1. process the model with the chosen parameters\n",
    "    # get_test_accuracy(lr, activation, dropout_rate, batch\n",
    "    #_size, max_poch, optimizer) -> test_accuracy\n",
    "    # 2. keep track of the test accuracy\n",
    "    # 3. if test accuracy greater than previous max\n",
    "  counter = 0\n",
    "\n",
    "  for lr in lrs:\n",
    "  #   print(f'lr {lr}')\n",
    "    for act in activations:\n",
    "  #     print(f'activation {act}')\n",
    "      for dropout_rate in dropout_rates:\n",
    "  #       print(f'dropout_rate {dropout_rate}')\n",
    "        for batch_size in batch_sizes:\n",
    "  #         print(f'batch_size {batch_size}')\n",
    "          for max_epoch in max_epochs:\n",
    "  #           print(f'max_epoch {max_epoch}')\n",
    "            for optimizer in optimizers:\n",
    "              if optimizer == 'Adam':\n",
    "  #               print(f'optimizer {optimizer}')\n",
    "                for decay in decay_rates:\n",
    "  #                 print(f'decay{decay}')\n",
    "                  for grad in ams_grad:\n",
    "  #                   print(f'grad: {grad}')\n",
    "                    counter += 1\n",
    "                    accuracy_test = get_test_accuracy(lr, dropout_rate, batch_size, max_epoch, act, optimizer, decay, ams_grad)\n",
    "                    print(f'Counter: {counter}')\n",
    "                    print(Fore.GREEN + \"Best Accuracy: {}\".format(test_accuracy_max))\n",
    "                    print(Fore.RED + \"Accuracy: {}\".format(accuracy_test))\n",
    "                    clear_output(wait=True)\n",
    "                    if accuracy_test > test_accuracy_max:\n",
    "                      print(Fore.GREEN + \"Best Accuracy: {}\".format(accuracy_test))\n",
    "                      clear_output(wait=True)\n",
    "                      test_accuracy_max = accuracy_test\n",
    "                      best_hyperparameters['lr'] = lr\n",
    "                      best_hyperparameters['activation'] = act\n",
    "                      best_hyperparameters['dropout_rate'] = dropout_rate\n",
    "                      best_hyperparameters['batch_size'] = batch_size\n",
    "                      best_hyperparameters['max_epoch'] = max_epoch\n",
    "                      best_hyperparameters['optimizer'] = optimizer\n",
    "                      best_hyperparameters['decay'] = decay\n",
    "                      best_hyperparameters['ams_grad'] = grad\n",
    "              elif optimizer == 'Adamax':\n",
    "  #               print(f'optimizer {optimizer}')\n",
    "                for decay in decay_rates:\n",
    "  #                 print(f'decay{decay}')\n",
    "                  counter += 1\n",
    "                  accuracy_test = get_test_accuracy(lr, dropout_rate, batch_size, max_epoch, act, optimizer, decay, 'null')\n",
    "                  print(f'Counter: {counter}')\n",
    "                  print(Fore.GREEN + \"Best Accuracy: {}\".format(test_accuracy_max))\n",
    "                  print(Fore.RED + \"Accuracy: {}\".format(accuracy_test))\n",
    "                  clear_output(wait=True)\n",
    "                  if accuracy_test > test_accuracy_max:\n",
    "                    print(Fore.GREEN + \"Best Accuracy: {}\".format(accuracy_test))\n",
    "                    clear_output(wait=True)\n",
    "                    test_accuracy_max = accuracy_test\n",
    "                    best_hyperparameters['lr'] = lr\n",
    "                    best_hyperparameters['activation'] = act\n",
    "                    best_hyperparameters['dropout_rate'] = dropout_rate\n",
    "                    best_hyperparameters['batch_size'] = batch_size\n",
    "                    best_hyperparameters['max_epoch'] = max_epoch\n",
    "                    best_hyperparameters['optimizer'] = optimizer\n",
    "                    best_hyperparameters['decay'] = decay\n",
    "                    best_hyperparameters['ams_grad'] = 'null'\n",
    "              elif optimizer == 'LBFGS':\n",
    "  #               print(f'optimizer {optimizer}')\n",
    "                counter += 1\n",
    "                accuracy_test = get_test_accuracy(lr, dropout_rate, batch_size, max_epoch, act, optimizer, 'null', 'null')\n",
    "                print(f'Counter: {counter}')\n",
    "                print(Fore.GREEN + \"Best Accuracy: {}\".format(test_accuracy_max))\n",
    "                print(Fore.RED + \"Accuracy: {}\".format(accuracy_test))\n",
    "                clear_output(wait=True)\n",
    "                if accuracy_test > test_accuracy_max:\n",
    "                  print(Fore.GREEN + \"Best Accuracy: {}\".format(accuracy_test))\n",
    "                  clear_output(wait=True)\n",
    "                  test_accuracy_max = accuracy_test\n",
    "                  best_hyperparameters['lr'] = lr\n",
    "                  best_hyperparameters['activation'] = act\n",
    "                  best_hyperparameters['dropout_rate'] = dropout_rate\n",
    "                  best_hyperparameters['batch_size'] = batch_size\n",
    "                  best_hyperparameters['max_epoch'] = max_epoch\n",
    "                  best_hyperparameters['optimizer'] = optimizer\n",
    "                  best_hyperparameters['decay'] = 'null'\n",
    "                  best_hyperparameters['ams_grad'] = 'null'\n",
    "                  # keep track of the hyperparameters in a dict\n",
    "                  # best_hyperparams['lr']\n",
    "                  # test_accuracy_max = test_accuracy\n",
    "\n",
    "grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "TnjZvhjhMNGh",
    "outputId": "0406f92a-524f-4b9c-fe36-c72b52d67e30"
   },
   "outputs": [],
   "source": [
    "print(f'Best Accuracy: {test_accuracy_max}')\n",
    "print(f'{best_hyperparameters}')\n",
    "test_accuracy_max = 0\n",
    "best_hyperparameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aQf22tsfjeT"
   },
   "outputs": [],
   "source": [
    "# torch.load(\"model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgSyH3b3f4RQ"
   },
   "source": [
    "**Kfold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "z_fwn5Vwf1vg",
    "outputId": "3becfd71-43ca-4176-f447-8d7d7a62a2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 10364.2341\t Val. loss: 1937.9811 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 9721.2144\t Val. loss: 3089.8762 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 9699.8065\t Val. loss: 3697.1359 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11698.9174\t Val. loss: 1652.6035 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10137.1855\t Val. loss: 2658.2072 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10099.4275\t Val. loss: 3400.0087 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 12140.0410\t Val. loss: 1310.8171 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10485.4195\t Val. loss: 2132.1900 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10484.6630\t Val. loss: 2166.9422 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11395.8436\t Val. loss: 1200.2266 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10870.5562\t Val. loss: 1347.2920 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10870.4862\t Val. loss: 1347.5011 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 10877.5898\t Val. loss: 1347.7262 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10868.9922\t Val. loss: 1387.5586 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10868.9356\t Val. loss: 1384.7281 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11046.6248\t Val. loss: 1218.1013 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10654.6675\t Val. loss: 1795.3784 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10654.7004\t Val. loss: 1796.6097 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11228.0417\t Val. loss: 1174.3331 \n",
      " Train Acc: 16.00\t Val Acc: 65.00\n",
      "Epoch: 51 \tTraining Loss: 11000.1478\t Val. loss: 1136.5517 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 11000.0878\t Val. loss: 1136.5504 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11050.1602\t Val. loss: 1107.8364 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 11008.3320\t Val. loss: 1225.7119 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 11008.4650\t Val. loss: 1227.2995 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 11072.4002\t Val. loss: 1140.7542 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10974.6892\t Val. loss: 1163.3347 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10974.6581\t Val. loss: 1164.1194 \n",
      " Train Acc: 19.00\t Val Acc: 0.00\n",
      "Epoch: 1 \tTraining Loss: 10816.4791\t Val. loss: 1356.5778 \n",
      " Train Acc: 20.00\t Val Acc: 0.00\n",
      "Epoch: 51 \tTraining Loss: 10722.1857\t Val. loss: 1668.2367 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n",
      "Epoch: 101 \tTraining Loss: 10722.3038\t Val. loss: 1668.7130 \n",
      " Train Acc: 23.00\t Val Acc: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_temp = scaler.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_temp, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds = KFold(10, False).split(x_temp)\n",
    "for _, (X_index, y_index) in enumerate(kfolds):\n",
    "    '''\n",
    "      Create tensors for our train / valid / test sets. \n",
    "      Need variable to accumulate gradients. \n",
    "      First we create tensor, then we will create variable \n",
    "    '''\n",
    "#     model = ANN(input_dim = 24, output_dim = 9, act='relu')\n",
    "    X_train = X[X_index]\n",
    "    y_train = y[X_index]\n",
    "    X_valid = X[y_index]\n",
    "    y_valid = y[y_index]\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "\n",
    "    # Numpy to Tensor Conversion (Test Set)\n",
    "    X_valid = torch.from_numpy(X_valid)\n",
    "    y_valid = torch.from_numpy(y_valid)\n",
    "    # Make torch datasets from train and test sets\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    valid = torch.utils.data.TensorDataset(X_valid,y_valid)\n",
    "\n",
    "    # Create train and test data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size = 64, shuffle = True)\n",
    "    # Define num epochs\n",
    "    epochs = 101\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "    # Some lists to keep track of loss and accuracy during each epoch\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    # Start epochs\n",
    "    for epoch in range(epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        # Set the training mode ON -> Activate Dropout Layers\n",
    "        model.train() # prepare model for training\n",
    "        # Calculate Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Load Train Tuples with Labels(Targets)\n",
    "        for data, target in train_loader:\n",
    "\n",
    "            # Convert our feature and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Training Accuracy \n",
    "            predicted = torch.max(output.data, 1)[1]        \n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average training loss over an epoch\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        # Avg Accuracy\n",
    "        train_accuracy = 100 * correct / float(total)\n",
    "\n",
    "        # Put them in their list\n",
    "        train_acc_list.append(train_accuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "        # Implement Validation like K-fold Cross-validation \n",
    "        # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "        model.eval() # Required for Evaluation/Test\n",
    "\n",
    "        # Calculate Test/Validation Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "\n",
    "                # Convert our features and labels to Variables to accumulate Gradients\n",
    "                data = Variable(data).float()\n",
    "                target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "                # Predict Output\n",
    "                output = model(data)\n",
    "\n",
    "                # Calculate Loss\n",
    "                loss = loss_fn(output, target)\n",
    "                val_loss += loss.item()*data.size(0)\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "                # Total number of labels\n",
    "                total += len(target)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == target).sum()\n",
    "\n",
    "        # calculate average training loss and accuracy over an epoch\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = 100 * correct/ float(total)\n",
    "\n",
    "        # Put them in their list\n",
    "        val_acc_list.append(val_accuracy)\n",
    "        val_loss_list.append(val_loss)\n",
    "        if epoch % 50 == 0:\n",
    "            # Print the Epoch and Training Loss Details with Validation Accuracy   \n",
    "            print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. loss: {:.4f} \\n Train Acc: {:.2f}\\t Val Acc: {:.2f}'.format(\n",
    "                epoch+1, \n",
    "                train_loss,\n",
    "                val_loss,\n",
    "                train_accuracy,\n",
    "                val_accuracy\n",
    "                ))\n",
    "            # save model if validation loss has decreased\n",
    "            if val_loss <= valid_loss_min:\n",
    "#                 print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "#                 valid_loss_min,\n",
    "#                 val_loss))\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "                valid_loss_min = val_loss\n",
    "            # Move to next epoch\n",
    "            epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2Kq-13AAeiV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CRwWJeUg3fk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 2457.5613231658936\n",
      "Test Accuracy tensor(20)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "accuracy_test = 0\n",
    "test_total = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "  # Convert our features and labels to Variables to accumulate Gradients\n",
    "  data = Variable(data).float()\n",
    "  target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "  # Predict Output\n",
    "  output = model(data)\n",
    "\n",
    "  # Calculate Loss\n",
    "  loss = loss_fn(output, target)\n",
    "  test_loss += loss.item()*data.size(0)\n",
    "  # Get predictions from the maximum value\n",
    "  predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "  # Total number of labels\n",
    "  test_total += len(target)\n",
    "\n",
    "  # Total correct predictions\n",
    "  correct += (predicted == target).sum()\n",
    " \n",
    "test_loss = np.mean(test_loss)\n",
    "accuracy_test = 100 * correct/ float(test_total)\n",
    "print('Test loss', test_loss)\n",
    "print('Test Accuracy', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qq33me1Qf14u"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJKKn4gmf2Au"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "PIp4emT2f2Ka",
    "outputId": "844c2497-2716-4387-8097-0226078db2a1"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "He3-2MuKkOay",
    "outputId": "3b692f8a-f353-428e-ba28-b58118950375"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predict = clf.predict(X_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsGZyennf2TQ"
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Mjzp0J14gBGW",
    "outputId": "624019a9-0dc2-40ce-9182-4019707108ed"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "(pd.Series(clf.feature_importances_, index=X_df.columns)\n",
    "   .nlargest(24)\n",
    "   .plot(kind='barh'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "n8v15GGEgBYa",
    "outputId": "b4d3927d-8415-4a09-f32b-569ee4bf9f65"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(x_temp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qyqLGpbZh7L2",
    "outputId": "7e29c2d2-61a2-4b69-93d5-51949bdc43f7"
   },
   "outputs": [],
   "source": [
    "predict = clf.predict(x_temp)\n",
    "accuracy_score(y, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uajEoFXSj-kb"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFd3yuK9lfPC"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(boosting_type='gbdt')\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "bom-MPDElqbY",
    "outputId": "e6bb92e7-b655-4a67-b2b5-9831fae1fe09"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LITE GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "model.fit(X_train, y_train)\n",
    "predict = model.predict(X_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "extensive_features_mlp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
